{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO_S8wDFIu0j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from agent import *\n",
        "from channel import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def R2P(x):\n",
        "    return abs(x), np.angle(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hd = Hd()\n",
        "hr = Hr()\n",
        "g = G()\n",
        "hd = R2P(hd())\n",
        "hr = R2P(hr())\n",
        "g = R2P(g())\n",
        "h = np.concatenate((np.array(hr).squeeze(), np.array(\n",
        "    hd).squeeze(), np.array(g).reshape(2, 500)), axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "W8PiLS-ZctKC",
        "outputId": "a53b0cbc-15d0-4e4e-eb6e-27a7a20e272a"
      },
      "outputs": [],
      "source": [
        "num_cell = 1\n",
        "num_base = 1\n",
        "num_irs = 1\n",
        "num_user = 1\n",
        "num_antenna_base = 10\n",
        "num_element_irs = 5*10\n",
        "num_antenna_user = 1\n",
        "num_slot = 1000\n",
        "Pmax_base_w = 5\n",
        "dist_base_irs_m = 48\n",
        "dist_base_user_m = 50\n",
        "dist_irs_user_m = 5\n",
        "sigma_linear = 8e-11\n",
        "\n",
        "num_actions = num_element_irs\n",
        "num_states = num_element_irs + 1\n",
        "batch_size = 300\n",
        "rewards = []\n",
        "avg_rewards = []\n",
        "agent = DDPGagent(num_actions, num_states, actor_learning_rate=1e-4,\n",
        "                  critic_learning_rate=1e-3, disc_fact=0.95, tau=0.005, max_memory_size=50000)\n",
        "noise = OUNoise(num_actions)\n",
        "noise.reset()\n",
        "\n",
        "hd = Hd()\n",
        "hr = Hr()\n",
        "g = G()\n",
        "hd = np.matrix(hd())\n",
        "hr = np.matrix(hr())\n",
        "g = np.matrix(g())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "state:shifts\n",
        "action:shifts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for episode in range(100):\n",
        "    snr_ = []\n",
        "    for _ in range(100):\n",
        "        angles = 2*np.pi*np.random.rand(num_actions)\n",
        "        shifts = Phi(angles)\n",
        "        snr_.append(SNR(hr, shifts, g, hd,  Pmax_base_w, sigma_linear))\n",
        "    if episode % 10 == 0:\n",
        "        print(episode)\n",
        "    # generate initail phase-shift on irs randomly\n",
        "    state = 2*np.pi*np.random.rand(num_actions)\n",
        "    shifts = Phi(state)\n",
        "    snr = SNR(hr, shifts, g, hd,  Pmax_base_w, sigma_linear)\n",
        "    state = np.concatenate((state, snr), axis=None).reshape(1, num_states)\n",
        "    episode_reward = 0\n",
        "    for step in range(num_slot):\n",
        "        action = agent.get_action(state)\n",
        "        action = noise.get_action(action)\n",
        "        # noise = np.random.normal(loc=0, scale=np.sqrt(10), size=(1, num_actions))\n",
        "        # action = action + noise\n",
        "        shifts = Phi(action.squeeze())\n",
        "        snr = SNR(hr, shifts, g, hd,  Pmax_base_w, sigma_linear)\n",
        "        next_state = np.concatenate(\n",
        "            (action, snr), axis=None).reshape(1, num_states)\n",
        "        reward = next_state[:, -1].item() - np.mean(snr)\n",
        "        agent.memory.push(state, action, reward, next_state)\n",
        "\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.update(batch_size)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "    rewards.append(episode_reward/num_slot)\n",
        "    avg_rewards.append(np.mean(rewards[-10:]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "shate:h\n",
        "action:shifts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cell = 1\n",
        "num_base = 1\n",
        "num_irs = 1\n",
        "num_user = 1\n",
        "num_antenna_base = 10\n",
        "num_element_irs = 5*10\n",
        "num_antenna_user = 1\n",
        "num_slot = 1000\n",
        "Pmax_base_w = 5\n",
        "dist_base_irs_m = 48\n",
        "dist_base_user_m = 50\n",
        "dist_irs_user_m = 5\n",
        "sigma_linear = 8e-11\n",
        "\n",
        "num_actions = num_element_irs\n",
        "num_states = 1120\n",
        "batch_size = 300\n",
        "rewards = []\n",
        "avg_rewards = []\n",
        "agent = DDPGagent(num_actions, num_states, actor_learning_rate=1e-5,\n",
        "                  critic_learning_rate=1e-4, disc_fact=0.95, tau=0.005, max_memory_size=50000)\n",
        "noise = OUNoise(num_actions)\n",
        "noise.reset()\n",
        "\n",
        "hd = Hd()\n",
        "hr = Hr()\n",
        "g = G()\n",
        "hd_ = R2P(hd())\n",
        "hr_ = R2P(hr())\n",
        "g_ = R2P(g())\n",
        "h = np.concatenate((np.array(hr_).squeeze(),np.array(hd_).squeeze(),np.array(g_).reshape(2,500)),axis=1)\n",
        "hd = np.matrix(hd())\n",
        "hr = np.matrix(hr())\n",
        "g = np.matrix(g())\n",
        "\n",
        "for episode in range(10):\n",
        "    if episode % 1 == 0:\n",
        "        print(episode)\n",
        "    # generate initail phase-shift on irs randomly\n",
        "    angles = 2*np.pi*np.random.rand(num_actions)\n",
        "    shifts = Phi(angles)\n",
        "    snr = SNR(hr, shifts, g, hd,  Pmax_base_w, sigma_linear)\n",
        "    episode_reward = 0\n",
        "    for step in range(num_slot):\n",
        "        state = h.reshape(1,1120)\n",
        "        # state = np.concatenate((state, snr), axis=None).reshape(1, num_states)\n",
        "        action = agent.get_action(state)\n",
        "        action = noise.get_action(action)\n",
        "        # noise = np.random.normal(loc=0, scale=np.sqrt(10), size=(1, num_actions))\n",
        "        # action = action + noise\n",
        "        shifts = Phi(action.squeeze())\n",
        "        snr = SNR(hr, shifts, g, hd,  Pmax_base_w, sigma_linear)\n",
        "        next_state = state\n",
        "        # next_state = np.concatenate((h.reshape(1,1120), snr), axis=None).reshape(1, num_states)\n",
        "        reward = next_state[:, -1].item()\n",
        "        agent.memory.push(state, action, reward, next_state)\n",
        "\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.update(batch_size)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "    rewards.append(episode_reward/num_slot)\n",
        "    avg_rewards.append(np.mean(rewards[-10:]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(rewards)\n",
        "plt.plot(avg_rewards)\n",
        "plt.plot()\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "snr = []\n",
        "for _ in range(100000):\n",
        "    state = 2*np.pi*np.random.rand(num_actions)\n",
        "    shifts = Phi(state)\n",
        "    snr.append(SNR(hr, shifts, g, hd,  Pmax_base_w, sigma_linear))\n",
        "print(np.mean(snr), max(snr), min(snr))\n",
        "plt.hist(snr, bins=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "snr = []\n",
        "for _ in range(1000):\n",
        "    hd = Hd()\n",
        "    hr = Hr()\n",
        "    g = G()\n",
        "    hd = np.matrix(hd())\n",
        "    hr = np.matrix(hr())\n",
        "    g = np.matrix(g())\n",
        "    state = 2*np.pi*np.random.rand(num_actions)\n",
        "    shifts = Phi(state)\n",
        "    snr.append(SNR(hr, shifts, g, hd,  Pmax_base_w, sigma_linear))\n",
        "print(np.mean(snr), max(snr), min(snr))\n",
        "plt.hist(snr, bins=50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DRL for IRS.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1afbf4dae76a4cbb0de72c2356a5fa5be655e6e843383971066724040fc78769"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
